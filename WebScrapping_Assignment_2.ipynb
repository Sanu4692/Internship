{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab358584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd256946",
   "metadata": {},
   "source": [
    "1\n",
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "\n",
    "1.first get the webpage https://www.naukri.com/\n",
    "2.Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3.Then click the search button.\n",
    "4.Then scrape the data for the first 10 jobs results you get.\n",
    "5.Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fab4ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experiance_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Gaussian Networks Private Limited\\n3.6\\n(24 Re...</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tcs Hiring For Senior Data Analyst (bfsi domain)</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>Tata Consultancy Services Ltd.\\n4.0\\n(23680 Re...</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Trigent Software\\n3.8\\n(159 Reviews)</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Trigent Software\\n3.8\\n(159 Reviews)</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business Data Analyst - Google Data Studio &amp; SQL</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AVE-Promagne</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram...</td>\n",
       "      <td>Virtusa Consulting Services Pvt Ltd\\n3.4\\n(111...</td>\n",
       "      <td>8-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tcs Hiring For MDM (master data management) Da...</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "      <td>Tata Consultancy Services Ltd.\\n4.0\\n(23680 Re...</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Analyst IDAM Services</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited\\n4.3\\n...</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst/Sr.Data Engineer</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>SYREN TECHNOLOGIES PRIVATE LIMITED</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SENIOR DATA ANALYST</td>\n",
       "      <td>Hyderabad/Secunderabad, Chennai, Bangalore/Ben...</td>\n",
       "      <td>McAfee Software (India) Pvt. Ltd\\n3.8\\n(137 Re...</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0                                Senior Data Analyst   \n",
       "1   Tcs Hiring For Senior Data Analyst (bfsi domain)   \n",
       "2                              Business Data Analyst   \n",
       "3                              Business Data Analyst   \n",
       "4   Business Data Analyst - Google Data Studio & SQL   \n",
       "5                                Senior Data Analyst   \n",
       "6  Tcs Hiring For MDM (master data management) Da...   \n",
       "7                  Senior Data Analyst IDAM Services   \n",
       "8                      Data Analyst/Sr.Data Engineer   \n",
       "9                                SENIOR DATA ANALYST   \n",
       "\n",
       "                                            Location  \\\n",
       "0              Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "1                       Chennai, Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5  Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram...   \n",
       "6                                 (WFH during Covid)   \n",
       "7                       Chennai, Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9  Hyderabad/Secunderabad, Chennai, Bangalore/Ben...   \n",
       "\n",
       "                                        Company_Name Experiance_Required  \n",
       "0  Gaussian Networks Private Limited\\n3.6\\n(24 Re...             3-5 Yrs  \n",
       "1  Tata Consultancy Services Ltd.\\n4.0\\n(23680 Re...            6-11 Yrs  \n",
       "2               Trigent Software\\n3.8\\n(159 Reviews)            5-10 Yrs  \n",
       "3               Trigent Software\\n3.8\\n(159 Reviews)            5-10 Yrs  \n",
       "4                                       AVE-Promagne             3-8 Yrs  \n",
       "5  Virtusa Consulting Services Pvt Ltd\\n3.4\\n(111...            8-12 Yrs  \n",
       "6  Tata Consultancy Services Ltd.\\n4.0\\n(23680 Re...            6-11 Yrs  \n",
       "7  GlaxoSmithKline Pharmaceuticals Limited\\n4.3\\n...             4-8 Yrs  \n",
       "8                 SYREN TECHNOLOGIES PRIVATE LIMITED             4-9 Yrs  \n",
       "9  McAfee Software (India) Pvt. Ltd\\n3.8\\n(137 Re...             6-8 Yrs  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing chromedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting Url\n",
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "search_bar=driver.find_element_by_id(\"qsb-keyword-sugg\")   #search bar\n",
    "search_bar.send_keys(\"Data Analyst\")  #writing on search bar\n",
    "search_loc=driver.find_element_by_id(\"qsb-location-sugg\") #finding job location bar\n",
    "search_loc.send_keys(\"Banglore\") # setting banglore location\n",
    "\n",
    "#click on search button:\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(4)\n",
    "# Extracting Data\n",
    "title=[]  #empty box\n",
    "loc=[]   #empty box\n",
    "cname=[] #empty box\n",
    "experiance=[] #empty box\n",
    "\n",
    "#titles\n",
    "titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles:\n",
    "    title.append(i.text)\n",
    "title=title[:10]\n",
    "\n",
    "# extracting location\n",
    "l=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span')\n",
    "for i in l:\n",
    "    loc.append(i.text)\n",
    "loc=loc[:10]\n",
    "\n",
    "#extracting companies names\n",
    "com=driver.find_elements_by_xpath('//div[@class=\"mt-7 companyInfo subheading lh16\"]')\n",
    "for i in com:\n",
    "    cname.append(i.text)\n",
    "cname=cname[:10]\n",
    "\n",
    "# extracting experiance data\n",
    "e=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in e:\n",
    "    experiance.append(i.text)\n",
    "experiance=experiance[:10]\n",
    "\n",
    "driver.close()\n",
    "#Making DataFrame\n",
    "Ans1=pd.DataFrame({})\n",
    "Ans1[\"Job_Title\"]=title\n",
    "Ans1[\"Location\"]=loc\n",
    "Ans1[\"Company_Name\"]=cname\n",
    "Ans1[\"Experiance_Required\"]=experiance\n",
    "Ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e5c3d",
   "metadata": {},
   "source": [
    "2\n",
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "\n",
    "1.first get the webpage https://www.naukri.com/\n",
    "2.Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3.Then click the search button.\n",
    "4.Then scrape the data for the first 10 jobs results you ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f10f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru, Mumbai (All Area...</td>\n",
       "      <td>Tata Consultancy Services Ltd.</td>\n",
       "      <td>Minumum 3 years of experience in Data Science/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Philips India Limited</td>\n",
       "      <td>Use predictive modeling to increase and optimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Brillio</td>\n",
       "      <td>A day in the Life of Data Scientist at Brillio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - I / II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Sharechat</td>\n",
       "      <td>Responsibilities\\nApply state of the art in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Allegis Services India Pvt. Ltd.</td>\n",
       "      <td>Must Have skill sets:\\n\\nExcellent knowledge o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart Labs</td>\n",
       "      <td>As a Senior Data Scientist for Walmart, you ll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>Responsibilities include:\\nDefining and evalua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Job_Title  \\\n",
       "0  Hiring For Data Scientist   \n",
       "1                         --   \n",
       "2   Associate Data Scientist   \n",
       "3             Data Scientist   \n",
       "4    Data Scientist - I / II   \n",
       "5             Data Scientist   \n",
       "6                         --   \n",
       "7                         --   \n",
       "8      Senior Data Scientist   \n",
       "9      Senior Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Chennai, Bangalore/Bengaluru, Mumbai (All Area...   \n",
       "1                                                 --   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                                 --   \n",
       "7                                                 --   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                       Company_Name  \\\n",
       "0    Tata Consultancy Services Ltd.   \n",
       "1                                --   \n",
       "2             Philips India Limited   \n",
       "3                           Brillio   \n",
       "4                         Sharechat   \n",
       "5  Allegis Services India Pvt. Ltd.   \n",
       "6                                --   \n",
       "7                                --   \n",
       "8                      Walmart Labs   \n",
       "9                            Airbnb   \n",
       "\n",
       "                                     Job_Description  \n",
       "0  Minumum 3 years of experience in Data Science/...  \n",
       "1                                                 --  \n",
       "2  Use predictive modeling to increase and optimi...  \n",
       "3  A day in the Life of Data Scientist at Brillio...  \n",
       "4  Responsibilities\\nApply state of the art in th...  \n",
       "5  Must Have skill sets:\\n\\nExcellent knowledge o...  \n",
       "6                                                 --  \n",
       "7                                                 --  \n",
       "8  As a Senior Data Scientist for Walmart, you ll...  \n",
       "9  Responsibilities include:\\nDefining and evalua...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing chrome drive\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "#loading url\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "time.sleep(3)\n",
    "\n",
    "#loacating search bar\n",
    "search_bar=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "\n",
    "\n",
    "#writing on search bar\n",
    "search_bar.send_keys(\"Data Scientist\")\n",
    "\n",
    "#finding job location bar\n",
    "search_loc=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "\n",
    "\n",
    "# setting banglore location\n",
    "search_loc.send_keys(\"Banglore\")\n",
    "\n",
    "#click on search button:\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#getting url\n",
    "url=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "\n",
    "urls=[]\n",
    "for i in url:\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "#emptybox\n",
    "titles=[]\n",
    "loc=[]\n",
    "cname=[]\n",
    "des=[]\n",
    "\n",
    "#extracting data\n",
    "for i in urls[:10]:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "  \n",
    "    #extractig jobtitles \n",
    "    try:\n",
    "        job=driver.find_element_by_xpath('//h1[@class=\"jd-header-title\"]')\n",
    "        titles.append(job.text)\n",
    "    except NoSuchElementException:\n",
    "        titles.append(\"--\")\n",
    "\n",
    "    #extractig locations\n",
    "    try:\n",
    "        k=driver.find_element_by_xpath('//div[@class=\"loc\"]')\n",
    "        loc.append(k.text)\n",
    "    except NoSuchElementException:\n",
    "        loc.append(\"--\")\n",
    "   \n",
    "   #extractig cname  \n",
    "    try:\n",
    "        l=driver.find_element_by_xpath(\"//a[@class='pad-rt-8']\")\n",
    "        cname.append(l.text)\n",
    "    except NoSuchElementException:\n",
    "        cname.append(\"--\")\n",
    "\n",
    "    #extracting Description\n",
    "    try:\n",
    "        z=driver.find_element_by_xpath('//div[@class=\"dang-inner-html\"]')\n",
    "        des.append(z.text)\n",
    "    except NoSuchElementException:\n",
    "        des.append(\"--\")\n",
    "\n",
    "driver.close()\n",
    "#Making DataFrame\n",
    "Ans2=pd.DataFrame({})\n",
    "Ans2[\"Job_Title\"]=titles\n",
    "Ans2[\"Location\"]=loc\n",
    "Ans2[\"Company_Name\"]=cname\n",
    "Ans2[\"Job_Description\"]=des\n",
    "Ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021d507",
   "metadata": {},
   "source": [
    "3\n",
    "In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required. The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "\n",
    "1.first get the webpage https://www.naukri.com/\n",
    "2.Enter “Data Scientist” in “Skill,Designations,Companies” field . Then click the search button.\n",
    "3.Then apply the location filter and salary filter by checking the respective boxes\n",
    "4.Then scrape the data for the first 10 jobs results you get.\n",
    "5.Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2601c465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experiance_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist / Data Analyst / Business Analy...</td>\n",
       "      <td>Ghaziabad, Faridabad, Delhi / NCR</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist/ Machine Learning Engineer</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Ahmedabad, Gurga...</td>\n",
       "      <td>Creative Hands HR Consultancy</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist / Data Analyst</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>CARS24</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist / Data Analyst / Business Analy...</td>\n",
       "      <td>Noida, New Delhi, Gurgaon/Gurugram</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida\\n(WFH during Covid)</td>\n",
       "      <td>R Systems International Ltd.</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram\\n(WFH during Covid)</td>\n",
       "      <td>PROCESS NINE TECHNOLOGIES PVT.LTD.</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Insurance</td>\n",
       "      <td>Noida, Gurgaon/Gurugram</td>\n",
       "      <td>Huquo Consulting Pvt. Ltd</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist - WFH - MIND Infotech</td>\n",
       "      <td>Pune, Chennai, Bangalore/Bengaluru, Delhi / NC...</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Junior Data Scientists &amp; Engineers</td>\n",
       "      <td>New Delhi, Delhi / NCR</td>\n",
       "      <td>PY Consultancy</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Data Scientist / Data Analyst / Business Analy...   \n",
       "1          Data Scientist/ Machine Learning Engineer   \n",
       "2                      Data Scientist / Data Analyst   \n",
       "3  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "4  Data Scientist / Data Analyst / Business Analy...   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                         Data Scientist - Insurance   \n",
       "8               Data Scientist - WFH - MIND Infotech   \n",
       "9                 Junior Data Scientists & Engineers   \n",
       "\n",
       "                                            Location  \\\n",
       "0                  Ghaziabad, Faridabad, Delhi / NCR   \n",
       "1  Hyderabad/Secunderabad, Pune, Ahmedabad, Gurga...   \n",
       "2                                   Gurgaon/Gurugram   \n",
       "3               Noida, Gurgaon/Gurugram, Delhi / NCR   \n",
       "4                 Noida, New Delhi, Gurgaon/Gurugram   \n",
       "5                          Noida\\n(WFH during Covid)   \n",
       "6               Gurgaon/Gurugram\\n(WFH during Covid)   \n",
       "7                            Noida, Gurgaon/Gurugram   \n",
       "8  Pune, Chennai, Bangalore/Bengaluru, Delhi / NC...   \n",
       "9                             New Delhi, Delhi / NCR   \n",
       "\n",
       "                               Company_Name Experiance_required  \n",
       "0                 GABA Consultancy services             0-0 Yrs  \n",
       "1             Creative Hands HR Consultancy             0-0 Yrs  \n",
       "2                                    CARS24             1-5 Yrs  \n",
       "3                 GABA Consultancy services             0-0 Yrs  \n",
       "4                 GABA Consultancy services             0-0 Yrs  \n",
       "5              R Systems International Ltd.             3-6 Yrs  \n",
       "6        PROCESS NINE TECHNOLOGIES PVT.LTD.             1-3 Yrs  \n",
       "7                 Huquo Consulting Pvt. Ltd             2-7 Yrs  \n",
       "8  MOTHERSONSUMI INFOTECH & DESIGNS LIMITED             3-7 Yrs  \n",
       "9                            PY Consultancy             0-3 Yrs  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "#calling site\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "#extraction preocess\n",
    "search_bar=driver.find_element_by_id(\"qsb-keyword-sugg\") #search bar\n",
    "search_bar.clear()  #clearing search bar\n",
    "search_bar.send_keys(\"Data Scientist\")   #inserting \"Data science\" in search bar\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click() #clicking on search button\n",
    "time.sleep(4)\n",
    "driver.find_element_by_xpath('//*[@title=\"Delhi / NCR\"]').click()   #setting location filter delhi\n",
    "time.sleep(4)\n",
    "driver.find_element_by_xpath('//*[@title=\"3-6 Lakhs\"]').click()   #setting salary\n",
    "time.sleep(4)\n",
    "#extracting the required data\n",
    "titles=[] #empty list for job_title\n",
    "loc=[] #empty list for location\n",
    "cname=[] #empty list for company name\n",
    "expr=[] #empty list for experiance_requires\n",
    "\n",
    "\n",
    "#job_title\n",
    "title=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title:\n",
    "    titles.append(i.text)\n",
    "titles=titles[:10]\n",
    "\n",
    "#location\n",
    "location=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in location:\n",
    "    loc.append(i.text)\n",
    "loc=loc[:10]\n",
    "\n",
    "#company_name\n",
    "cn=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in cn:\n",
    "    cname.append(i.text)\n",
    "cname=cname[:10]\n",
    "\n",
    "#experiance_required\n",
    "exr=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in exr:\n",
    "    expr.append(i.text)\n",
    "expr=expr[:10]\n",
    "\n",
    "driver.close()\n",
    "#Making DataFrame\n",
    "Ans3=pd.DataFrame({})\n",
    "Ans3[\"Job_Title\"]=titles\n",
    "Ans3[\"Location\"]=loc\n",
    "Ans3[\"Company_Name\"]=cname\n",
    "Ans3[\"Experiance_required\"]=expr\n",
    "Ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a615618",
   "metadata": {},
   "source": [
    "4\n",
    "Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company. This task will be done in following steps:\n",
    "\n",
    "1.first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2.Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "3.Then click the search button.\n",
    "4.Then scrape the data for the first 10 jobs results you get in the above shown page. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3f76327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing chromedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting Url\n",
    "url=\"https://www.glassdoor.co.in/Job/index.htm\"\n",
    "driver.get(url)\n",
    "search_bar=driver.find_element_by_id(\"KeywordSearch\")   #search bar\n",
    "search_bar.send_keys(\"Data Scientist\")  #writing on search bar\n",
    "search_loc=driver.find_element_by_id(\"LocationSearch\") #finding job location bar\n",
    "search_loc.send_keys(\"Noida\") # setting Noida location\n",
    "\n",
    "#click on search button:\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\")\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(4)\n",
    "# Extracting Data\n",
    "cname=[]  #empty box\n",
    "postd=[]   #empty box\n",
    "rating=[] #empty box\n",
    "\n",
    "#Company name\n",
    "titles=driver.find_elements_by_xpath('//div[@class=\"d-flex justify-content-between align-items-start\"]/a')\n",
    "for i in titles:\n",
    "    cname.append(i.text)\n",
    "cname=cname[:10]\n",
    "\n",
    "# extracting rating\n",
    "r=driver.find_elements_by_xpath('//span[@class=\"css-19pjha7 e1cjmv6j1\"]')\n",
    "for i in r:\n",
    "    rating.append(i.text)\n",
    "rating=rating[:10]\n",
    "\n",
    "#extracting no.of day before posted\n",
    "com=driver.find_elements_by_xpath('//div[@class=\"d-flex align-items-end pl-std css-mi55ob\"]')\n",
    "for i in com:\n",
    "    postd.append(i.text)\n",
    "postd=postd[:10]\n",
    "\n",
    "driver.close()\n",
    "#Making DataFrame\n",
    "Ans4=pd.DataFrame({})\n",
    "Ans4[\"company_name\"]=cname\n",
    "Ans4[\"Rating\"]=rating\n",
    "Ans4[\"Posted\"]=postd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709441bd",
   "metadata": {},
   "source": [
    "5\n",
    "Write a python program to scrape the salary data for Data Scientist designation in Noida location. You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary. The above task will be, done as shown in the below steps:\n",
    "\n",
    "1.first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2.Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3.Click the search button. 4.Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary and rating of the company. 5..Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95e79d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>No. of salaries</th>\n",
       "      <th>Avg salaries</th>\n",
       "      <th>Minimum salaries</th>\n",
       "      <th>Maxmium salaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Company, No. of salaries, Avg salaries, Minimum salaries, Maxmium salaries]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing chromedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting Url\n",
    "url=\"https://www.glassdoor.co.in/Salaries/index.htm\"\n",
    "driver.get(url)\n",
    "search_bar=driver.find_element_by_id(\"KeywordSearch\")   #search bar\n",
    "search_bar.send_keys(\"Data Scientist\")  #writing on search bar\n",
    "search_loc=driver.find_element_by_id(\"LocationSearch\") #finding job location bar\n",
    "search_loc.send_keys(\"Noida\") # setting Noida location\n",
    "\n",
    "#click on search button:\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\")\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(6)\n",
    "# Extracting Data\n",
    "cname=[]  #empty box\n",
    "nsalaries=[]   #empty box\n",
    "avgsalaries=[] #empty box\n",
    "minmaxsalary=[] #empty box\n",
    "\n",
    "#Company name\n",
    "titles=driver.find_elements_by_xpath('//p[@class=\"m-0 \"]')\n",
    "for i in titles:\n",
    "    cname.append(i.text)\n",
    "cname=cname[:10]\n",
    "\n",
    "# extracting number of salaries\n",
    "n=driver.find_elements_by_xpath('//p[@class=\"css-1uyte9r css-1kuy7z7 m-0 \"]')\n",
    "for i in n:\n",
    "    nsalaries.append(i.text)\n",
    "nsalaries=nsalaries[:10]\n",
    "\n",
    "#extracting average salaries\n",
    "a=driver.find_elements_by_xpath('//div[@class=\"col-2 d-none d-md-flex flex-row justify-content-end\"]')\n",
    "for i in a:\n",
    "    avgsalaries.append(i.text.replace(\"\\n\",\"\"))\n",
    "avgsalaries=avgsalaries[:10]\n",
    "\n",
    "# extracting minimum salaries\n",
    "mi=driver.find_elements_by_xpath('//div[@class=\"common__RangeBarStyle__values d-flex justify-content-between \"]')\n",
    "for i in mi:\n",
    "    minmaxsalary.append(i.text)\n",
    "minmaxsalary=minmaxsalary[:10]\n",
    "#seprating minimum and maximum salary\n",
    "l =minmaxsalary\n",
    "split_list =[i.split() for i in l]\n",
    "flat_list = [item for sublist in split_list for item in sublist]\n",
    "minsalary=flat_list[::2]\n",
    "maxsalary=flat_list[1::2]\n",
    "\n",
    "driver.close()\n",
    "#Making DataFrame\n",
    "Ans5=pd.DataFrame({})\n",
    "Ans5[\"Company\"]=cname\n",
    "Ans5[\"No. of salaries\"]=nsalaries\n",
    "Ans5[\"Avg salaries\"]=avgsalaries\n",
    "Ans5[\"Minimum salaries\"]=minsalary\n",
    "Ans5[\"Maxmium salaries\"]=maxsalary\n",
    "Ans5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cfbec",
   "metadata": {},
   "source": [
    "6\n",
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "1.Brand\n",
    "2.Product Description\n",
    "3.Price\n",
    "4.Discount % To scrape the data you have to go through following steps:\n",
    "5.Go to flipkart webpage by url https://www.flipkart.com/\n",
    "6.Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "7.after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "8.after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it 5.Now scrape data from this page as usual\n",
    "9.repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "100669bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I Flash</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Rectangular...</td>\n",
       "      <td>₹137</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I Flash</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹132</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SHAAH COLLECTIONS</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹198</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹295</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹284</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>I Flash</td>\n",
       "      <td>Mirrored, UV Protection, Riding Glasses, Other...</td>\n",
       "      <td>₹198</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SHAAH COLLECTIONS</td>\n",
       "      <td>UV Protection, Gradient Wayfarer, Clubmaster S...</td>\n",
       "      <td>₹331</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection, Gradient, Mirrored, Riding Glas...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>68% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Round Sunglasses (55)</td>\n",
       "      <td>₹237</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>I Flash</td>\n",
       "      <td>UV Protection, Mirrored, Gradient Round Sungla...</td>\n",
       "      <td>₹170</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Brand                                Product_description  \\\n",
       "0             I Flash  UV Protection, Polarized, Mirrored Rectangular...   \n",
       "1             I Flash                UV Protection Round Sunglasses (54)   \n",
       "2   SHAAH COLLECTIONS  Mirrored, UV Protection Wayfarer Sunglasses (F...   \n",
       "3           Elligator  UV Protection, Gradient Rectangular Sunglasses...   \n",
       "4      kingsunglasses              UV Protection Aviator Sunglasses (54)   \n",
       "..                ...                                                ...   \n",
       "95            I Flash  Mirrored, UV Protection, Riding Glasses, Other...   \n",
       "96  SHAAH COLLECTIONS  UV Protection, Gradient Wayfarer, Clubmaster S...   \n",
       "97          ROYAL SON  UV Protection, Gradient, Mirrored, Riding Glas...   \n",
       "98             PIRASO                UV Protection Round Sunglasses (55)   \n",
       "99            I Flash  UV Protection, Mirrored, Gradient Round Sungla...   \n",
       "\n",
       "   Price Discount  \n",
       "0   ₹137  86% off  \n",
       "1   ₹132  86% off  \n",
       "2   ₹198  88% off  \n",
       "3   ₹295  88% off  \n",
       "4   ₹284  89% off  \n",
       "..   ...      ...  \n",
       "95  ₹198  83% off  \n",
       "96  ₹331  80% off  \n",
       "97  ₹474  68% off  \n",
       "98  ₹237  85% off  \n",
       "99  ₹170  85% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing chromedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting Url\n",
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "time.sleep(2)\n",
    "search_bar=driver.find_element_by_xpath('//*[@title=\"Search for products, brands and more\"]')   #search bar\n",
    "search_bar.click()\n",
    "search_bar.send_keys(\"sunglasses\")  #writing on search bar\n",
    "\n",
    "#click on search button:\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(4)\n",
    "# Extracting Data\n",
    "brand=[]  #empty box\n",
    "pdes=[]   #empty box\n",
    "price=[] #empty box\n",
    "discount=[] #empty box\n",
    "\n",
    "for i in range(0,3):   #chossing top 3 pages\n",
    "    for j in driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]'):   #brand\n",
    "        brand.append(j.text)\n",
    "    for l in driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]'):      #description\n",
    "        pdes.append(l.text)\n",
    "\n",
    "    for c in driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]'):   #price\n",
    "        price.append(c.text)\n",
    "\n",
    "    for e in driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]'):   #discount\n",
    "        discount.append(e.text)\n",
    "    driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()      #next page\n",
    "    time.sleep(5)\n",
    "\n",
    "#selecting top 100 \n",
    "brand=brand[:100]\n",
    "pdes=pdes[:100]\n",
    "price=price[:100]\n",
    "discount=discount[:100]\n",
    "\n",
    "driver.close()\n",
    "#Making DataFrame\n",
    "Ans6=pd.DataFrame({})\n",
    "Ans6[\"Brand\"]=brand\n",
    "Ans6[\"Product_description\"]=pdes\n",
    "Ans6[\"Price\"]=price\n",
    "Ans6[\"Discount\"]=discount\n",
    "Ans6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d5bc7",
   "metadata": {},
   "source": [
    "7\n",
    "Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace. As shown in the above page you have to scrape the tick marked attributes. These are\n",
    "\n",
    "1.Rating\n",
    "2.Review_summary\n",
    "3.Full review You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d4cdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_summaru</th>\n",
       "      <th>Detail_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>So far it’s been an AMAZING experience coming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>i11 is worthy to buy, too much happy with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating       Review_summaru  \\\n",
       "0       5            Brilliant   \n",
       "1       5       Simply awesome   \n",
       "2       5  Best in the market!   \n",
       "3       5     Perfect product!   \n",
       "4       5    Worth every penny   \n",
       "..    ...                  ...   \n",
       "95      5            Fabulous!   \n",
       "96      5        Great product   \n",
       "97      4          Good choice   \n",
       "98      5    Worth every penny   \n",
       "99      5   Highly recommended   \n",
       "\n",
       "                                       Detail_summary  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  This is my first iOS phone. I am very happy wi...  \n",
       "96  Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "97  So far it’s been an AMAZING experience coming ...  \n",
       "98  i11 is worthy to buy, too much happy with the ...  \n",
       "99  iphone 11 is a very good phone to buy only if ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing chromedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting Url\n",
    "url=\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\"\n",
    "driver.get(url)\n",
    "driver.find_element_by_xpath('//div[@class=\"_3UAT2v _16PBlm\"]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Extracting Data\n",
    "rating=[]  #empty box\n",
    "rsum=[]   #empty box\n",
    "fsum=[] #empty box\n",
    "\n",
    "\n",
    "for i in range(0,11):   #chossing top 3 pages\n",
    "    for j in driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]'):   #Rating\n",
    "        rating.append(j.text)\n",
    "    for l in driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]'):      #Review_summary\n",
    "        rsum.append(l.text)\n",
    "    for c in driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]'):   #Detail_summary\n",
    "        fsum.append(c.text)\n",
    "    driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()      #next page\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "driver.close()\n",
    "#selecting top 100 \n",
    "rating=rating[:100]\n",
    "rsum=rsum[:100]\n",
    "fsum=fsum[:100]\n",
    "\n",
    "#Making DataFrame\n",
    "Ans7=pd.DataFrame({})\n",
    "Ans7[\"Rating\"]=rating\n",
    "Ans7[\"Review_summaru\"]=rsum\n",
    "Ans7[\"Detail_summary\"]=fsum\n",
    "Ans7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa51c9",
   "metadata": {},
   "source": [
    "8\n",
    "Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker :\n",
    "\n",
    "1.Brand\n",
    "2.Product Description\n",
    "3.Price\n",
    "4.discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "141efa04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹356</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Combo Pack Of 4 Casual Shoes Loafer Shoes Snea...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Longwalk</td>\n",
       "      <td>Men Boxer Sneakers For Men</td>\n",
       "      <td>₹236</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Perfect &amp; Affordable Combo Pack of 02 Pairs Sn...</td>\n",
       "      <td>₹599</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASTEROID</td>\n",
       "      <td>Original Luxury Branded Fashionable Men's Casu...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>RODDICK SHOES</td>\n",
       "      <td>Combo Men Pack of 2 Loafers Shoes Sneakers For...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Absolute comfort</td>\n",
       "      <td>SM-322 Sneakers For Men</td>\n",
       "      <td>₹198</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>believe</td>\n",
       "      <td>Fashion Outdoor Canvas Casual Light Weight Lac...</td>\n",
       "      <td>₹449</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>New Collection of Combo Pack of 2 Stylish Shoe...</td>\n",
       "      <td>₹240</td>\n",
       "      <td>51% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Airland</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹264</td>\n",
       "      <td>47% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand                                Product_description  \\\n",
       "0              Magnolia                                   Sneakers For Men   \n",
       "1                BRUTON  Combo Pack Of 4 Casual Shoes Loafer Shoes Snea...   \n",
       "2              Longwalk                         Men Boxer Sneakers For Men   \n",
       "3                Chevit  Perfect & Affordable Combo Pack of 02 Pairs Sn...   \n",
       "4              ASTEROID  Original Luxury Branded Fashionable Men's Casu...   \n",
       "..                  ...                                                ...   \n",
       "95        RODDICK SHOES  Combo Men Pack of 2 Loafers Shoes Sneakers For...   \n",
       "96     Absolute comfort                            SM-322 Sneakers For Men   \n",
       "97              believe  Fashion Outdoor Canvas Casual Light Weight Lac...   \n",
       "98  World Wear Footwear  New Collection of Combo Pack of 2 Stylish Shoe...   \n",
       "99              Airland                                   Sneakers For Men   \n",
       "\n",
       "   Price Discount  \n",
       "0   ₹356  64% off  \n",
       "1   ₹474  86% off  \n",
       "2   ₹236  52% off  \n",
       "3   ₹599  66% off  \n",
       "4   ₹499  75% off  \n",
       "..   ...      ...  \n",
       "95  ₹474  52% off  \n",
       "96  ₹198  60% off  \n",
       "97  ₹449  55% off  \n",
       "98  ₹240  51% off  \n",
       "99  ₹264  47% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing chromedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting Url\n",
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "time.sleep(2)\n",
    "search_bar=driver.find_element_by_xpath('//*[@title=\"Search for products, brands and more\"]')   #search bar\n",
    "search_bar.click()\n",
    "search_bar.send_keys(\"sneakers\")  #writing on search bar\n",
    "\n",
    "#click on search button:\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(4)\n",
    "# Extracting Data\n",
    "brand=[]  #empty box\n",
    "pdes=[]   #empty box\n",
    "price=[] #empty box\n",
    "discount=[] #empty box\n",
    "\n",
    "for i in range(0,4):   #chossing top 4 pages\n",
    "    for j in driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]'):   #brand\n",
    "        brand.append(j.text)\n",
    "    for l in driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]'):      #description\n",
    "        pdes.append(l.text)\n",
    "\n",
    "    for c in driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]'):   #price\n",
    "        price.append(c.text)\n",
    "\n",
    "    for e in driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]'):   #discount\n",
    "        discount.append(e.text)\n",
    "    driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()      #next page\n",
    "    time.sleep(5)\n",
    "\n",
    "#selecting top 100 \n",
    "brand=brand[:100]\n",
    "pdes=pdes[:100]\n",
    "price=price[:100]\n",
    "discount=discount[:100]\n",
    "\n",
    "driver.close()\n",
    "#Making DataFrame\n",
    "Ans8=pd.DataFrame({})\n",
    "Ans8[\"Brand\"]=brand\n",
    "Ans8[\"Product_description\"]=pdes\n",
    "Ans8[\"Price\"]=price\n",
    "Ans8[\"Discount\"]=discount\n",
    "Ans8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae60b4",
   "metadata": {},
   "source": [
    "9\n",
    "Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbf0ab8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men BlackTraining or Gym Shoes</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men Cell Fraction Fade Running</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 8099Rs. 8999(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 8999Rs. 9999(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Mesh Hybrid Fuego Running</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Tommy Hilfiger</td>\n",
       "      <td>Men Suede Sneakers</td>\n",
       "      <td>Rs. 7599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Women Leather Pumps</td>\n",
       "      <td>Rs. 7192Rs. 8990(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Unisex Mercedes F1 Sneakers</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Unisex Pacer Future Sneakers</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Unisex Mirage Sport Trainers</td>\n",
       "      <td>Rs. 8999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Brand                Product_description  \\\n",
       "0                   Puma     Men BlackTraining or Gym Shoes   \n",
       "1                   Puma     Men Cell Fraction Fade Running   \n",
       "2           Hush Puppies  Men Solid Leather Formal Slip-Ons   \n",
       "3           Hush Puppies  Men Solid Leather Formal Slip-Ons   \n",
       "4                   Puma          Mesh Hybrid Fuego Running   \n",
       "..                   ...                                ...   \n",
       "95        Tommy Hilfiger                 Men Suede Sneakers   \n",
       "96  Heel & Buckle London                Women Leather Pumps   \n",
       "97       PUMA Motorsport        Unisex Mercedes F1 Sneakers   \n",
       "98                  Puma       Unisex Pacer Future Sneakers   \n",
       "99                  Puma       Unisex Mirage Sport Trainers   \n",
       "\n",
       "                        Price  \n",
       "0                    Rs. 6999  \n",
       "1                    Rs. 6999  \n",
       "2   Rs. 8099Rs. 8999(10% OFF)  \n",
       "3   Rs. 8999Rs. 9999(10% OFF)  \n",
       "4                    Rs. 6999  \n",
       "..                        ...  \n",
       "95                   Rs. 7599  \n",
       "96  Rs. 7192Rs. 8990(20% OFF)  \n",
       "97                   Rs. 7999  \n",
       "98                   Rs. 6999  \n",
       "99                   Rs. 8999  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing chromedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "#getting Url\n",
    "url=\"https://www.myntra.com/shoes\"\n",
    "driver.get(url)\n",
    "time.sleep(4)\n",
    "driver.find_element_by_xpath(\"//label[@class='common-customCheckbox']\").click()        \n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\").click()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "time.sleep(4)\n",
    "# Extracting Data\n",
    "brand=[]  #empty box\n",
    "sdes=[]   #empty box\n",
    "price=[] #empty box\n",
    "\n",
    "\n",
    "for i in range(0,3):   #chossing top 4 pages\n",
    "    for j in driver.find_elements_by_xpath('//h3[@class=\"product-brand\"]'):   #brand\n",
    "        brand.append(j.text)\n",
    "    for l in driver.find_elements_by_xpath('//h4[@class=\"product-product\"]'):      #description\n",
    "        sdes.append(l.text)\n",
    "\n",
    "    for c in driver.find_elements_by_xpath('//div[@class=\"product-price\"]'):   #price\n",
    "        price.append(c.text)\n",
    "\n",
    "   \n",
    "    driver.find_element_by_xpath('//li[@class=\"pagination-next\"]').click()      #next page\n",
    "    time.sleep(3)\n",
    "\n",
    "#selecting top 100 \n",
    "brand=brand[:100]\n",
    "sdes=sdes[:100]\n",
    "price=price[:100]\n",
    "\n",
    "driver.close()\n",
    "#Making DataFrame\n",
    "Ans9=pd.DataFrame({})\n",
    "Ans9[\"Brand\"]=brand\n",
    "Ans9[\"Product_description\"]=sdes\n",
    "Ans9[\"Price\"]=price\n",
    "Ans9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4861e",
   "metadata": {},
   "source": [
    "10\n",
    "Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "\n",
    "1.title\n",
    "2.Ratings\n",
    "3.Price As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34b0c709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HP Envy 11th Gen Core i7 Processor 13.3-inch (...</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "      <td>₹1,23,350.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acer Swift 5 14\" (35.56cms) Full HD IPS Displa...</td>\n",
       "      <td>3.7 out of 5</td>\n",
       "      <td>₹94,999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i7-1...</td>\n",
       "      <td>4.2 out of 5</td>\n",
       "      <td>₹57,990.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenovo IdeaPad S540 11th Gen Intel Core i7 13....</td>\n",
       "      <td>4.3 out of 5</td>\n",
       "      <td>₹77,990.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP Pavilion 13(2021) 11th Gen Intel Core i7 La...</td>\n",
       "      <td>5 out of 5</td>\n",
       "      <td>₹92,990.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fujitsu UH-X 11th Gen Intel Core i7 13.3” (33....</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "      <td>₹1,07,990.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lenovo IdeaPad Gaming 3 Intel Core i7 10th Gen...</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "      <td>₹73,990.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fujitsu UH-X 11th Gen Intel i7 Core 13.3” (33....</td>\n",
       "      <td>4.2 out of 5</td>\n",
       "      <td>₹92,990.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MSI GF75 Thin, Intel i7-10750H, 17.3\" (43.9 cm...</td>\n",
       "      <td>4 out of 5</td>\n",
       "      <td>₹74,990.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HP Pavilion (2021) Thin &amp; Light 11th Gen Core ...</td>\n",
       "      <td>4.3 out of 5</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title        Rating  \\\n",
       "0  HP Envy 11th Gen Core i7 Processor 13.3-inch (...  4.1 out of 5   \n",
       "1  Acer Swift 5 14\" (35.56cms) Full HD IPS Displa...  3.7 out of 5   \n",
       "2  Mi Notebook Horizon Edition 14 Intel Core i7-1...  4.2 out of 5   \n",
       "3  Lenovo IdeaPad S540 11th Gen Intel Core i7 13....  4.3 out of 5   \n",
       "4  HP Pavilion 13(2021) 11th Gen Intel Core i7 La...    5 out of 5   \n",
       "5  Fujitsu UH-X 11th Gen Intel Core i7 13.3” (33....  4.1 out of 5   \n",
       "6  Lenovo IdeaPad Gaming 3 Intel Core i7 10th Gen...  4.4 out of 5   \n",
       "7  Fujitsu UH-X 11th Gen Intel i7 Core 13.3” (33....  4.2 out of 5   \n",
       "8  MSI GF75 Thin, Intel i7-10750H, 17.3\" (43.9 cm...    4 out of 5   \n",
       "9  HP Pavilion (2021) Thin & Light 11th Gen Core ...  4.3 out of 5   \n",
       "\n",
       "          Price  \n",
       "0  ₹1,23,350.00  \n",
       "1    ₹94,999.00  \n",
       "2    ₹57,990.00  \n",
       "3    ₹77,990.00  \n",
       "4    ₹92,990.00  \n",
       "5  ₹1,07,990.00  \n",
       "6    ₹73,990.00  \n",
       "7    ₹92,990.00  \n",
       "8    ₹74,990.00  \n",
       "9            --  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing chrome drive\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "#loading url\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "time.sleep(3)\n",
    "\n",
    "#loacating search bar\n",
    "search_bar=driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "\n",
    "\n",
    "#writing on search bar\n",
    "search_bar.send_keys(\"Laptops\")\n",
    "\n",
    "#click on search button:\n",
    "search_btn=driver.find_element_by_id(\"nav-search-submit-button\")\n",
    "search_btn.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#i7 filter\n",
    "filter_bi7=driver.find_elements_by_xpath(\"//span[@class='a-size-base a-color-base']\")\n",
    "for i in filter_bi7:\n",
    "    if i.text==\"Intel Core i7\":\n",
    "        i.click()\n",
    "        break\n",
    "\n",
    "#getting url\n",
    "url=driver.find_elements_by_xpath('//a[@class=\"a-link-normal a-text-normal\"]')\n",
    "\n",
    "urls=[]\n",
    "for i in url:\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "#emptybox\n",
    "titles=[]\n",
    "rating=[]\n",
    "price=[]\n",
    "\n",
    "#extracting data\n",
    "for i in urls[:10]:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "  \n",
    "    #extractig titles \n",
    "    try:\n",
    "        t=driver.find_element_by_xpath('//span[@class=\"a-size-large product-title-word-break\"]')\n",
    "        titles.append(t.text)\n",
    "    except NoSuchElementException:\n",
    "        titles.append(\"--\")\n",
    "\n",
    "    #extractig rating\n",
    "    try:\n",
    "        k=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-base\"]')\n",
    "        rating.append(k.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append(\"--\")\n",
    "   \n",
    "   #extractig price \n",
    "    try:\n",
    "        l=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-price priceBlockBuyingPriceString']\")\n",
    "        price.append(l.text)\n",
    "    except NoSuchElementException:\n",
    "        price.append(\"--\")\n",
    "driver.close()\n",
    "  \n",
    "\n",
    "#Making DataFrame\n",
    "Ans10=pd.DataFrame({})\n",
    "Ans10[\"Job_Title\"]=titles\n",
    "Ans10[\"Rating\"]=rating\n",
    "Ans10[\"Price\"]=price\n",
    "Ans10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4c070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
